#!/usr/bin/env node

/**
 * Script principal de scraping y sincronizaciÃ³n
 * 
 * Uso:
 *   node scraper-main.js                    # Scrapea y sincroniza
 *   node scraper-main.js --dry-run          # Solo scrapea, guarda JSON
 *   MAX_PAGES=10 node scraper-main.js       # Scrapea 10 pÃ¡ginas
 */

const ProductScraper = require('./scrapers/product-scraper');
const DatabaseSync = require('./sync/database-sync');
const config = require('./config/scraper.config');

async function main() {
  const isDryRun = process.argv.includes('--dry-run');
  const scraper = new ProductScraper();
  const dbSync = new DatabaseSync();

  try {
    // Inicializar scraper
    await scraper.initialize();

    // Scrapear productos
    const products = await scraper.scrapeAll();

    // Guardar en archivo
    await dbSync.saveToFile(products);

    // Sincronizar con BD (si no es dry-run)
    if (!isDryRun && config.database.syncEnabled) {
      await dbSync.syncAll(products);
    } else if (isDryRun) {
      await dbSync.logger.log('\nâš ï¸  Modo DRY-RUN: No se sincronizÃ³ con la base de datos');
    }

    // EstadÃ­sticas finales
    const totalImages = products.reduce((sum, p) => sum + (p.images?.length || 0), 0);
    await dbSync.logger.log('\n' + '='.repeat(50));
    await dbSync.logger.log('ğŸ‰ PROCESO COMPLETADO');
    await dbSync.logger.log(`   ğŸ“¦ Productos: ${products.length}`);
    await dbSync.logger.log(`   ğŸ–¼ï¸  ImÃ¡genes: ${totalImages}`);
    await dbSync.logger.log(`   ğŸ“„ Archivo: ${config.output.jsonFile}`);
    await dbSync.logger.log('='.repeat(50));

  } catch (error) {
    console.error('\nâŒ ERROR FATAL:', error);
    await dbSync.logger.error('Error fatal en scraper', error);
    process.exit(1);

  } finally {
    await scraper.close();
    await dbSync.close();
  }
}

// Ejecutar
main();

# Scraper Profesional - Droppers

Sistema modular de scraping con sincronizaciÃ³n automÃ¡tica a base de datos.

## ğŸ“ Estructura

```
scripts/
â”œâ”€â”€ scraper-main.js              # Script principal
â”œâ”€â”€ config/
â”‚   â””â”€â”€ scraper.config.js        # ConfiguraciÃ³n centralizada
â”œâ”€â”€ scrapers/
â”‚   â””â”€â”€ product-scraper.js       # LÃ³gica de scraping
â”œâ”€â”€ sync/
â”‚   â””â”€â”€ database-sync.js         # SincronizaciÃ³n con BD
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helpers.js               # Utilidades
â”œâ”€â”€ data/                        # JSONs generados
â””â”€â”€ logs/                        # Logs del proceso
```

## ğŸš€ Uso

### InstalaciÃ³n
```bash
npm install puppeteer axios
```

### EjecuciÃ³n bÃ¡sica
```bash
# Scrapea y sincroniza con BD
node scripts/scraper-main.js

# Solo scrapea (sin tocar BD)
node scripts/scraper-main.js --dry-run

# Scrapear 10 pÃ¡ginas
MAX_PAGES=10 node scripts/scraper-main.js

# Ver el navegador (modo debug)
HEADLESS=false node scripts/scraper-main.js
```

## âš™ï¸ ConfiguraciÃ³n

Edita `scripts/config/scraper.config.js`:

```javascript
// Control de trÃ¡fico
rateLimit: {
  requestDelay: 2000,    // 2seg entre productos
  pageDelay: 5000,       // 5seg entre pÃ¡ginas
  maxRetries: 3,         // Reintentos
}

// ImÃ¡genes
images: {
  minWidth: 400,         // ResoluciÃ³n mÃ­nima
  downloadEnabled: true, // Descargar localmente
  downloadPath: './public/products'
}

// Base de datos
database: {
  syncEnabled: true,     // Auto-sincronizar
  updateExisting: true   // Actualizar existentes
}
```

## ğŸ” Variables de entorno

Crea un archivo `.env`:

```env
DROPPERS_EMAIL=tu-email@gmail.com
DROPPERS_PASSWORD=tu-password
MAX_PAGES=5
HEADLESS=true
```

## ğŸ¯ CaracterÃ­sticas

- âœ… Rate limiting automÃ¡tico (evita bloqueos)
- âœ… Reintentos con backoff exponencial
- âœ… ValidaciÃ³n de resoluciÃ³n de imÃ¡genes
- âœ… Descarga local de imÃ¡genes
- âœ… SincronizaciÃ³n inteligente (crea/actualiza)
- âœ… Logs detallados
- âœ… Manejo robusto de errores
- âœ… Procesamiento por lotes

## ğŸ“Š Logs

Los logs se guardan en:
- `scripts/logs/scraper.log` - Log general
- `scripts/logs/errors.log` - Solo errores
- `scripts/data/productos-sync.json` - Datos scrapeados

## ğŸ”§ Troubleshooting

**Error de login:**
```bash
# Verifica credenciales en config/scraper.config.js
```

**Bloqueo por rate limiting:**
```bash
# Aumenta los delays en la configuraciÃ³n
requestDelay: 3000  # 3 segundos
pageDelay: 10000    # 10 segundos
```

**ImÃ¡genes no se descargan:**
```bash
# Verifica permisos en la carpeta public/products
mkdir -p public/products
```

## ğŸ“ Notas

- El scraper respeta el trÃ¡fico del sitio con delays inteligentes
- Las imÃ¡genes se filtran automÃ¡ticamente (min 400x400px)
- Los productos se identifican por SKU para evitar duplicados
- Modo dry-run para probar sin afectar la BD
